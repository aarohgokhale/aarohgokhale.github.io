---
title: Exploring Hamiltonians in Control Theory
date: 2025-11-07T17:55:24-06:00
showtoc: true
draft: true
summary: Continuing on from my exploration of the use of physics-inspired concepts in machine learning and optimizer theory from earlier in the year, here I explore yet another area in which physics-inspired mathematics is used for the purpose of solving a problem in mathematics.
---
#+LATEX_HEADER: \usepackage{amsmath}


In an [[/technical/continuous_time_optimizers/][article from earlier in the year]], I explored one of the ways in which the ideas of [[https://en.wikipedia.org/wiki/Hamiltonian_mechanics][Hamiltonian mechanics]] have been used in the context of mathematics and computer science, specifically in optimizer theory. The continuous time limit of the update rules used for gradient-based optimizers such as the heavy ball method and Adam, results in a system of differential equations. This system can be framed as a set of Hamiltonianian dynamics equations in order to gain physical intuition. Here, I will explore yet another area in mathematics where Hamiltonian mechanics play a key role. Specifically, we will be looking at the [[https://en.wikipedia.org/wiki/Hamiltonian_(control_theory)][notion of Hamiltonians in control theory]], where the dynamics are influenced not just by the position and momentum, but also by a control function $u$. Then, we will look at how machine learning models are being used to /learn/ the Hamiltonian dynamics of a control system. A big difference between this article and the previous one is that the previous article talked about how Hamiltonians can be used to better understand optimizers which then eventually train machine learning models, whereas this article talks about how on the flip side, machine learning models can be used to learn Hamiltonians, which then help solve problems in optimal control.

* The Control-theoretic Hamiltonian
We begin by looking at how the notion of Hamiltonians is applicable in a control-theoretic setting. Let $\mathbf{x}(t)$ represent a generalized coordinate vector in $\mathbb{R}^m$ as a function of time $t$. Let $\mathbf{x}(t)$ evolve according to

\begin{eqnarray*}
\mathbf{x}(t) &=& f(\mathbf{x}(t), u(t), t) \\
\mathbf{x}(0) &=& \mathbf{x}_0
\end{eqnarray*}

where $u(t)$ is a control function that assigns a control to each time $t$. The optimal control problem is an optimization over the choice of functions $u$ such that a given objective/loss function is optimized over a time interval $[t_0, t_1]$. In particular, we want to optimize the value of the integral of the loss over this interval. If $L(\mathbf{x}(t), u(t))$ is the objective at time $t$ and $\Psi(\mathbf{x})$ represents the additional cost of ending up at state $\mathbf{x}$, we may want to solve

$$\underset{u(t)}{\text{argmax}} \int_{t_0}^{t_1} L(\mathbf{x}(t), u(t))dt + \Psi(\mathbf{x}(t_1))$$

subject to initial conditions and the control dynamics defined by $f$. On its face this is an optimization problem on function spaces, a problem in [[https://en.wikipedia.org/wiki/Calculus_of_variations][the calculus of variations]]. Lev Pontryagin's work in the 1950s led to a reformulation that reduces the optimization over function space to a pointwise optimization in Euclidean space. Let $H$, called the /Hamiltonian/, be defined as follows:

$$H(\mathbf{x}(t), u(t), \lambda(t), t) := \lambda^T (t) \cdot f(\mathbf{x}(t), u(t)) + L(\mathbf{x}(t), u(t))$$

where $\lambda(t)$ is a [[https://en.wikipedia.org/wiki/Lagrange_multiplier][Lagrange multiplier]] function. [[https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle][Pontraygin's maximum principle]] states that if there exists an optimal control $u^*$, optimal trajectory $\mathbf{x}^*$, and corresponding $\lambda^*$, then for all $t \in [t_0, t_1]$, and possible controls $u$, 

$$H(\mathbf{x}(t), u, \lambda(t), t) \leq H(\mathbf{x}^{*}(t), u^*(t), \lambda^*(t), t)$$

This means that finding the optimum control $u$ at time $t$ can be reduced to optimizing the Hamiltonian at time $t$. 

** Connection to the usual Hamiltonian
The dynamics of the control Hamiltonian are analogus to the Hamiltonian dynamics typically founded in conservative energy systems in physics. Namely, we have the following system of differential equations:

* Symplectic ODE-Net 
The first paper we will be looking at, titled [[https://arxiv.org/abs/1909.12077]["Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control"]], by Zhong et. al., proposes a neural network model for learning the dynamics of a physical system through observed trajectories. The crucial detail in this model is that it enforces a Hamiltonian prior, which drives solutions to be consistent with the knowledge of physical dynamics.
