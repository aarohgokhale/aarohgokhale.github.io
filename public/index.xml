<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Aaroh&#39;s Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Aaroh&#39;s Blog</description>
    <generator>Hugo -- 0.140.2</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Jan 2025 21:39:20 -0600</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Physics View of Function Optimization</title>
      <link>http://localhost:1313/technical/continuous_time_optimizers/</link>
      <pubDate>Tue, 21 Jan 2025 21:39:20 -0600</pubDate>
      <guid>http://localhost:1313/technical/continuous_time_optimizers/</guid>
      <description>An exploration of how the common problem of function optimization over $\mathbb{R}^d$ can be viewed through the lens of physics, and in particular, Hamiltonian mechanics. This perspective is taken by many modern papers on optimization algorithms, and none of them seem to go too deep into the motivations.</description>
      <content:encoded><![CDATA[
<p>
In my recent readings, I have encountered several papers that convert the usual <a href="https://optimization.cbe.cornell.edu/index.php?title=Momentum">momentum-based</a> <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">optimization</a> algorithms such as <a href="https://pages.cs.wisc.edu/~brecht/cs726docs/HeavyBallLinear.pdf">Polyak&#39;s Heavy Ball Method</a> or <a href="https://optimization.cbe.cornell.edu/index.php?title=Adam">Adam</a> to their <em>continuous-time</em> variants in order to perform some form of analysis on them.</p>
<p>
Here, I would like to explore the broad notions used in these papers.</p>
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
Continuous Time Forms
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<p>One of the interesting notions in many of these papers is to look at the <em>continuous-time</em> forms of momentum based optimizers. The main problem I tend to face is that these papers usually just don&#39;t explain how they arrived to a particular system of differential equations for the continuous time form of a particular optimizer. A friend of mine suggested that it could be just looking at the differential equations for which the optimizer update matches <a href="https://en.wikipedia.org/wiki/Euler_method">Euler&#39;s Method</a> for solving ODEs. For example, the usual vanilla gradient descent rule is</p>
<p>
$$x_{t^{(i + 1)}} = x_{t^{(i)}} - \eta \cdot \nabla f (x_{t^{(i)}})$$</p>
<p>
where $f$ is the loss function and $\eta$ is some learning rate. If we instead let $\eta = t^{(i + 1)} - t^{(i)} = \Delta t$, we get</p>
<p>
$$x_{t^{(i + 1)}} = x_{t^{(i)}} - \nabla f (x_{t^{(i)}}) \Delta t$$</p>
<p>
which is indeed the recurrence relation of Euler&#39;s method for the differential equation</p>
<p>
$$x&#39; = -\nabla f(x)$$</p>
<p>
The classic momentum update rule described by Polyak is given by</p>
<p>
$$p_{t^{(i)}} = \beta \cdot p_{t^{(i - 1)}} - (1 - \beta)\nabla f(x_{t^{(i)}})$$</p>
<p>
$$x_{t^{(i + 1)}} = x_{t^{(i)}} - \alpha \cdot p_{t^{(i)}}$$</p>
<p>
We can try to write this as a <em>system</em> of two differential equations. The first equation isn&#39;t yet in the typical Euler form, so we rewrite it as</p>
<p>
$$p_{t^{(i)}} = p_{t^{(i - 1)}} + (\beta - 1)\cdot p_{t^{(i - 1)}} - (1 - \beta) \cdot \nabla f(x_{t^{(i)}})$$</p>
<p>
Factoring out $(\beta - 1)$ from the last two terms, we get</p>
<p>
$$p_{t^{(i)}} = p_{t^{(i - 1)}} + (\beta - 1) \cdot (p_{t^{(i - 1)}} + \nabla f(x_{t^{(i)}}))$$</p>
<p>
Then if $\beta = \Delta t + 1$, we have the standard form of Euler&#39;s method for the differential equation</p>
<p>
$$p&#39; = p + \nabla f(x)$$</p>
<p>
For the other equation, we have a simpler differential equation, $x&#39; = -p$.</p>
<p>
We can then put them together by replacing $p&#39;$ with $-x&#39;&#39;$, giving us</p>
<p>
$$x&#39;&#39; = -p - \nabla f (x)$$</p>
<p>
Since $-p = x&#39;$,</p>
<p>
$$x&#39;&#39; = -x&#39; - \nabla f(x)$$</p>
<p>
Though this still seems a bit odd and contrived to me, it does seem that the momentum optimizer is the correct Euler method reccurence for this system of differential equations, and should therefore work to find the true trajectory defined by the differential equations when $p + \nabla f(x)$ is Lipschitz. Of course, just because the approximation is correct doesn&#39;t mean that the trajectory is of any use. It must be shown that $x(t)$ converges a stationary point of the loss function $f$. </p>
</div>
</div>
<div id="outline-container-headline-2" class="outline-2">
<h2 id="headline-2">
Hamiltonian Dynamics
</h2>
<div id="outline-text-headline-2" class="outline-text-2">
<p>
The paper entitled <a href="https://arxiv.org/abs/1809.05042">&#34;Hamiltonian Descent Methods&#34;</a> by Maddison et al. notes that this system mirrors a Hamiltonian dynamics system from physics, which is typically stated as</p>
<p>
$$x&#39;_t = \nabla_p \mathcal{H}(x_t, p_t) = \nabla k(p_t)$$</p>
<p>
$$p&#39;_t = -\nabla_x \mathcal{H}(x_t, p_t) = -\nabla f(x)$$</p>
<p>
where $\mathcal{H}(x_t, p_t)$ is the total in the system as a function of the position and momentum, $k$ is the kinetic energy as a function of the momentum, and $f$ is the potential energy as a function of the position. If we let $k(p_t) = \langle p_t, p_t \rangle / 2$, the equations are nearly identical to the previously continuous time version of th e momentum optimizer (note that since I&#39;m using the version where momentum is <em>subtracted</em>, the sign is flipped), with the caveat that the the second differential equation also has the term $p$, which the paper talks about as a &#34;disspiation field&#34; of the form $p&#39;_t = -\gamma \cdot p_t$, thus giving us a final field $(x&#39;_t, p&#39;_t) = F(x_t, p_t) + G(x_t, p_t)$ where $F$ is the Hamiltonian field $(\nabla k (p_t), -\nabla f(x))$ and $G$ is the dissipation field $(0, -\gamma \cdot p_t)$. The paper calls this a &#34;conformal&#34; Hamiltonian field/system. It also allows for a notion of a more general conformal Hamiltonian system, where $k$ is allowed to be an arbitrary nonnegative convex function with $k(0) = 0$ and $\gamma &gt; 0$. </p>
<p>
This poses the problem of optimization as a problem of computing the trajectory of a particle $x$ placed in a force field defined by the loss function $f$ starting at some position $x_0$ with some velocity $p_0$, where there also exists a dissipation force that scales with the particle&#39;s velocity and makes it an important factor in deciding where the particle will go next. The solutions to the system of differential equations remains in the set $\{(x, p)  : \mathcal{H}(x, p) = H_0\}$ for conservative forces $\nabla f$. However, when we add a disspitation force, such as the one defined by the momentum term in our optimizer, the total energy of the system decreases over time. </p>
<p>
A result from this paper is that given sufficiently nice conditions to the system, there exists a unique solution $(x_t, p_t)$ given initial conditions $(x_0, p_0)$ and that the position function $x$ converges to a stationary point of $f$.</p>
<div id="outline-container-headline-3" class="outline-3">
<h3 id="headline-3">
Conditions
</h3>
<div id="outline-text-headline-3" class="outline-text-3">
<p>The specific conditions for existence are as follows:</p>
<ul>
<li>$k$ is nonnegative and convex with $k(0) = 0$ (I stated this above as the domain of functions that $k$ can be chosen from).</li>
<li>$\nabla f$ and $\nabla k$ are continuous. </li>
<li>$\mathcal{H}$ is <em>radially unbounded</em>: $\mathcal{H}(x, p) \to \infty$ as $||(x, p)||_{2} \to \infty$. This notation is a little unclear to me. The best guess I can make is that for any $\epsilon &gt; 0$, there exists $\delta &gt; 0$ such that for all $(x, p)$, $||(x, p)||_2 &gt; \delta \implies \mathcal{H}(x, p) &gt; \epsilon$.</li>
</ul>
<p>For uniquness, the additional condition that $\nabla f$ and $\nabla k$ are continuously differentiable is imposed.</p>
<p>
For convergence to a stationary point of $f$, given a solution $(x_t, p_t)$ to the system with initial conditions $(x_0, p_0) = (x, p)$, the following conditions are imposed:</p>
<ul>
<li>$f$ and $k$ are continuously differentiable</li>
<li>$k$ is strictly convex with a minimum $k(0) = 0$</li>
<li>$\mathcal{H}$ is radially unbounded</li>
<li>$f$ is bounded bel, though it might not be so useful for those that are only really familiar with the mathematics of it. </li>
</ul>
<p>Given the above conditions, the paper shows that $||\nabla f(x_t)||_2 \to 0$.</p>
<p>
Looking at this optimization problem from a physics perspective is perhaps insightful for people who have a strong understanding of and intuition for physics, though it might not be so useful for those that are only really familiar with the mathematics of it. </p>
</div>
</div>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>The NixOS Dual Boot Nightmare</title>
      <link>http://localhost:1313/reviews/nixos/</link>
      <pubDate>Thu, 19 Dec 2024 00:01:00 -0600</pubDate>
      <guid>http://localhost:1313/reviews/nixos/</guid>
      <description>Dual booting NixOS with another linux distro has been a major pain point for me when trying out this OS for the first time. Here I go into why it is so inconvenient to set up dual booting in NixOS.</description>
      <content:encoded><![CDATA[<p>
<img src="/nixos.png" alt="/nixos.png" title="/nixos.png" /></p>
<p>
I recently installed <a href="https://nixos.org/">NixOS</a> on a small 180 GiB partition on my SSD in order to get a taste of what this &#34;trendy&#34; (of course, by the time I installed NixOS, the peak of its hype online was several months in the past) distribution had to offer. The idea of a declarative package management style does seem quite intriguing, since it means that your operating system configuration, including the packages you want installed and even the subconfigurations of those packages, could be stored in a single file that could conveniently be moved around different machines, allowing you to immediately get going on any new NixOS installation in <em>minutes</em>. This seems to be of great practical use and much simpler than the usual process of installing a linux distribution,  and then installing all the necessary packages one by one. That process is slow, tedious, and prone to error. You&#39;ll often forget to install a small package, like <a href="https://gitlab.com/chinstrap/gammastep">gammastep</a>, for example, and go for days on end without realizing that you are destroying your sleep because of the blue light coming from the screen. Of course, I&#39;m sure there&#39;s a way to streamline this process so that it&#39;s less tedious and less prone to error, but it would probably take more work than learning how to use NixOS. NixOS also comes with a full blowing programming language, Nix, so perhaps it could pay dividends if one learns how to use it.</p>
<p>
While the concept of NixOS sounds very interesting and innovative, the actual experience hasn&#39;t been smooth, to say the least. Upon installation, I decided to install all my favorite software like emacs, neovim, sway, and kitty. So far, so good—though I would&#39;ve liked it if they had bundled in some vi-like editor with the installation, since vim motion diehards like me can&#39;t seem to edit anything without them; they included <em>firefox</em> for crying out loud. I did run into several issues while trying to get emacs to run with my config files, though I don&#39;t know whether to blame that on emacs or NixOS, so I will give it the benefit of the doubt. The next step was to make it easy to boot into either Arch or NixOS as I wished, since I still planned to mainly use Arch, which did have the significantly larger partition. Initially, NixOS installed <a href="https://systemd.io/BOOT/">systemd-boot</a>, which did not have an entry for Arch. I then booted into Arch and reconfigured <a href="https://www.gnu.org/software/grub/">grub</a>, hoping to get a NixOS entry. No luck. I looked at the <code class="verbatim">/boot/</code> directory, which is where my EFI partition was mounted. Aha! NixOS stores its kernel and <a href="https://en.wikipedia.org/wiki/Initial_ramdisk">initrd</a> in a separate directory in <code class="verbatim">/boot/</code> called <code class="verbatim">/boot/kernels/</code>. Perhaps this is why the simple command <code class="verbatim">grub-mkconfig -o /boot/grub/grub.cfg</code> didn&#39;t find the NixOS kernel. I tried to find simple ways to configure grub to look for kernels in a specific directory to no avail—call it a skill issue. I then used my laptop&#39;s boot menu to start the previously installed <code class="verbatim">systemd-boot</code> instead and booted into NixOS to see whether I could configure the dual boot from there instead. First, I removed the line that enabled systemd-boot. After reading some forums and blogs online, I found out a configuration that was suggested to work:</p>
<figure>
<div class="src src-nix">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-nix" data-lang="nix"><span style="display:flex;"><span>boot<span style="color:#f92672">.</span>loader <span style="color:#960050;background-color:#1e0010">=</span> {
</span></span><span style="display:flex;"><span>  grub <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    device <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;nodev&#34;</span>;
</span></span><span style="display:flex;"><span>    enable <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>;
</span></span><span style="display:flex;"><span>    useOSProber <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>;
</span></span><span style="display:flex;"><span>    efiSupport <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>;
</span></span><span style="display:flex;"><span>  };
</span></span><span style="display:flex;"><span>  efi<span style="color:#f92672">.</span>canTouchEfiVariables <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>;
</span></span><span style="display:flex;"><span>};</span></span></code></pre></div>
</div>
<figcaption>
Credit: <a href="https://www.codyhiar.com/blog/how-to-dual-boot-nixos-and-arch-linux/">https://www.codyhiar.com/blog/how-to-dual-boot-nixos-and-arch-linux/</a>
</figcaption>
</figure>
<p>
This configuration did seem to work initially, since when I looked at the menu entries in <code class="verbatim">/boot/grub/grub.cfg</code> there was an entry created for Arch linux with a seemingly correct path to the kernel and initramfs. But when I tried to boot into Arch through its menu entry in grub, I got an error that said that the linux kernel image could not be found. This really confused me, so I went back into NixOS and tried to rebuild the configuration again. The Arch entry was generated according to the stdout of <code class="verbatim">nixos-rebuild</code>, so I didn&#39;t bother to check <code class="verbatim">/boot/grub/grub.cfg</code>. When I rebooted and pressed enter on the entry for Arch, it didn&#39;t panic because it couldn&#39;t find a kernel. It seemed to be working! …and it booted into NixOS. Wait, what??! As is turned out, while the output of <code class="verbatim">nixos-rebuild</code> <em>did</em> say that Arch linux was found on the correct partition, it wrote the menu entry for Arch with the kernel for NixOS! Why though? I still don&#39;t know the answer, but one possible explanation is that for some reason it just blindly looked in the <code class="verbatim">/boot/kernels/</code> directory and used whatever kernel it found. I don&#39;t even think this theory makes that much sense, but its all I have for now. Based on this theory, I decided to try putting my Arch kernel and initramfs in the <code class="verbatim">/boot/kernels/</code> directory—without backing it up. Big Mistake! As soon as I ran <code class="verbatim">nixos-rebuild</code>, NixOS saw those extra files and decided that they were garbage due for cleaning, and erased them. I guess it makes sense, since without some sort of garbage cleaning hook, we might get a situation like Gentoo, where the kernel images just keep piling up in your <code class="verbatim">/boot/</code> directory until it becomes impossible to navigate, at which point you have to clean it up manually.</p>
<p>
Now I was left with no way to immediately boot into my Arch installation. I created a live USB with the standard Arch installation ISO, chrooted into my partition, installed the kernel again, and ran <code class="verbatim">grub-mkconfig</code> for good measure. I booted into Arch and stopped using NixOS for a few days to just focus on my studies. When I returned to NixOS, I decided to try one more thing: I deleted my <code class="verbatim">/boot/grub/grub.cfg</code> file and ran <code class="verbatim">nixos-rebuild --install-bootloader switch</code>. Viola! The new <code class="verbatim">grub.cfg</code> file did indeed have an entry for Arch with the correct kernel and initramfs, just like it did when I initially installed grub on NixOS. Once again, I rebooted and pressed enter on the Arch menu entry. This time, instead of saying that the kernel image couldn&#39;t be found, it said that the initramfs couldn&#39;t found. I thought this was extraordinarily strange. Once again, I booted into NixOS and tried running <code class="verbatim">nixos-rebuild</code>. Again, however, it replaced the Arch kernel with the NixOS kernel in the menu entry for Arch. So I deleted <code class="verbatim">grub.cfg</code> <em>again</em> and ran <code class="verbatim">nixos-rebuild</code>, and rebooted. It finally worked! I was booted into Arch. I still had no idea what was causing NixOS to overwrite the Arch kernel with NixOS kernel when the <code class="verbatim">grub.cfg</code> file existed before rebuilding the system.</p>
<p>
After forgetting about this issue and going back to configuring my NixOS install, I decided it was time to rebuild the system again to make my new changes (mainly to the display manager) current. Once again, <code class="verbatim">grub.cfg</code> was overwritten to have the Arch kernel replaced by the NixOS kernel. This was incredibly frustrating. This meant that no matter what, every time I wanted to rebuild my system, I was forced to delete <code class="verbatim">grub.cfg</code> and rebuild it again to get the menu entries correctly configured. I did some more searching but couldn&#39;t find anything at all about <code class="verbatim">grub.cfg</code> being written incorrectly when dual booting. I also found a forum post that claimed that <code class="verbatim">os-prober</code> only worked to detect Windows on NixOS, and it was therefore advisable to use systemd-boot instead if you wanted to dual boot with another linux distribution. Searching for solutions for dual booting with systemed-boot was a mostly unsuccessful endeavor, and I ended up finding some hacky solutions that required manually adding the efi entry for Arch to the systemd-boot menu which I didn&#39;t feel like doing. There is also the solution of just using F9 to go into the boot menu and manually starting either OS from a separate bootloader, but this isn&#39;t ideal because timing the F9 key press and then waiting for it to slowly load into the UEFI boot menu is just annoying.</p>
<p>
On a slightly different note, trying to get NixOS to disable display managers has also been really frustrating. When I disable sddm, it automatically falls back on lightdm. When I try to disable the display manager service as a whole, it does not work. When I try to use the recommended method of suppressing display managers, which is to set <code class="verbatim">startx</code> as your display manager in the configuration, it still starts lightdm. I have currently given up on this and just gone back to using sddm, which comes with its own issues on NixOS, for example being unable to login if I type on the sddm login screen on my external monitor instead of my laptop monitor. </p>
<p>
Overall, I have certainly had more pleasant experiences when setting up and using other distributions for the first time. Fedora, for example, didn&#39;t give me nearly as much trouble. I get it, though, NixOS is still more of an experimental distro with a small user base and patchy documentation. The core idea is still interesting, and I do see a good use case for it. I don&#39;t think I&#39;ll be daily driving it any time soon, but I will keep experimenting with it, and maybe eventually consider moving to it once I&#39;m satisfied with my own knowledge of it and with the maturity of the community, documentation, and software ecosystem. I would definitely recommend trying out NixOS, even just within a VM, to get a feeling for what this paradigm of package management has to offer.</p>
]]></content:encoded>
    </item>
    <item>
      <title>A Summary of &#34;End-to-End Differentiable Proving&#34;</title>
      <link>http://localhost:1313/technical/end-to-end-diff-prove/</link>
      <pubDate>Sat, 30 Nov 2024 09:11:30 -0600</pubDate>
      <guid>http://localhost:1313/technical/end-to-end-diff-prove/</guid>
      <description>Here, I summarize and try to explain in detail what I learned from the paper entitled &amp;#34;End-to-End Differentiable Proving&amp;#34; by Rocktäschel and Riedel.</description>
      <content:encoded><![CDATA[
<p>
Here, I summarize and try to explain in detail what I learned from the paper entitled <a href="https://arxiv.org/abs/1705.11040">&#34;End-to-End Differentiable Proving&#34;</a> by Rocktäschel and Riedel.</p>
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
Main Ideas
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<p>The main idea of this paper is to combine strategies from automated symbolic reasoning and learned vector representations symbolic entities to get a hybrid model of theorem proving, where proof search is enhanced by indicating that a proof is more likely to succeed if the entities within the goal have high similarity to entities in known proofs.</p>
<div id="outline-container-headline-2" class="outline-3">
<h3 id="headline-2">
Basics
</h3>
<div id="outline-text-headline-2" class="outline-text-3">
<p>The first barrier to entry in this paper is understanding the logic programming framework being used. Theorems are posed as queries to a database, proving a theorem involves systematically substituting the terms in the query until something that already exists in the database is reached, at which point the query is considered a success. If nothing is found, it fails.</p>
<div id="outline-container-headline-3" class="outline-4">
<h4 id="headline-3">
Definitions:
</h4>
<div id="outline-text-headline-3" class="outline-text-4">
<ul>
<li><strong>atom</strong>: An <em>atom</em> consists of a <em>predicate</em> and a list of terms. For example, something like $[\text{isCoprimeWith}, 8, 9]$. More generally, $[R, t_1, t_2, \ldots, t_n]$, where $R$ is an $n$-ary relation called the predicate. In this paper, a term can be either a <em>constant</em> or a <em>variable</em>. The example they use is $[\text{grandfatherOf}, Q, \text{BART}]$, where $\text{grandfatherOf}$ is the predicate, $Q$ is a variable, and $\text{BART}$ is a constant.</li>
<li><strong>rule</strong>: A <em>rule</em> is a structure of the form $H \mathrel{:-} \mathbb{B}$, where $H$ is an atom called the <em>head</em> and $\mathbb{B}$ is a possibly empty conjuction of atoms called the <em>body</em> of the rule. From my understanding, a rule of the form $q \mathrel{:-} [p_1, p_2, \ldots, p_N]$ describes the implication $p_1 \land p_2 \land \ldots \land p_N \rightarrow q$. A rule with no free variables (all the variables are universally quantified) is called a <em>ground rule</em>, and a ground rule with no body is called a <em>fact</em>.</li>
<li><strong>substitution set</strong>: A <em>substitution set</em> is a set of the form $\psi = \{X_1/t_1, \ldots, X_n/t_n\}$, which represents an assignment of free variables $X_1, \ldots, X_n$ to terms $t_1, \ldots, t_n$ respectively. Applying a substitution to an atom replaces all occurrences of variables in the substitution set with their repsective terms.</li>
</ul>
</div>
</div>
<div id="outline-container-headline-4" class="outline-4">
<h4 id="headline-4">
Backward Chaining Algorithm:
</h4>
<div id="outline-text-headline-4" class="outline-text-4">
<p>The algorithm used to prove a statement or to The basic idea of the algorithm is as follows: a function called OR is applied to the query (the goal). It iterates through the set of all rules and finds a <a href="https://en.wikipedia.org/wiki/Unification_(computer_science)">unification</a> of the goal with the rule&#39;s head (by trying to substitute the variables in either formula to match each other, and by making sure that when both terms are constants that they are equal). If OR is succesful in finding a unification, it calls a function called AND, which then proves all the atoms in the body of that rule (since each rule is an implication based on a conjuction of premises). AND uses the substitution set that was used to unify the goal with the rule head, and applies it to the subgoals (the atoms in the body). It then calls OR on the subgoals one by one to prove them.</p>
<p>
As someone with no experience in prolog and this framework of thinking, I found the pseudocode for the backward chaining algorithm a little bit cryptic. Below I transliterate the pseudocode from the appendix of the paper in just plain text (the original had $\LaTeX$):</p>
<div class="src src-text">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span> or(G, S) = [S&#39; | S&#39; in and(B, unify(H, G, S)) for H :- B in K]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> and(_, FAIL) = FAIL
</span></span><span style="display:flex;"><span> and([], S) = S
</span></span><span style="display:flex;"><span> and(G : bigG, S) = [S&#39;&#39; | S&#39;&#39; in and(bigG, S&#39;) for S&#39; in or(substitute(G, S), S)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> unify(_,_,FAIL) = FAIL
</span></span><span style="display:flex;"><span> unify([],[],S) = S
</span></span><span style="display:flex;"><span> unify([],_,_) = FAIL
</span></span><span style="display:flex;"><span> unify(_,[],_) = FAIL
</span></span><span style="display:flex;"><span> unify(h : H, g : G, S) = unify(H, G, S + {h/g} if h in V, 
</span></span><span style="display:flex;"><span>                                      S + {g/h} if g in V and h not in V,
</span></span><span style="display:flex;"><span>                                      S         if g = h
</span></span><span style="display:flex;"><span>                                      FAIL      otherwise)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> substitute([], _) = []
</span></span><span style="display:flex;"><span> substitute(g : G, S) = x if g/x in S, g otherwise : substitute(G,S)</span></span></code></pre></div>
</div>
</div>
</div>
<div id="outline-container-headline-5" class="outline-4">
<h4 id="headline-5">
Explanation: 
</h4>
<div id="outline-text-headline-5" class="outline-text-4">
<p>The first function, <code class="verbatim">or</code>, is collecting sets <code class="verbatim">S&#39;</code> such that they belong to the result of taking applying <code class="verbatim">and</code> to the bodies, <code class="verbatim">B</code>, of the rule heads, <code class="verbatim">H</code>, that manage to be unified with the goal, <code class="verbatim">G</code>. The <code class="verbatim">and</code> function, when given a set of subgoals, called <code class="verbatim">bigG,</code> and an existing substitution set <code class="verbatim">S</code>, goes through the subgoals one by one and applies <code class="verbatim">or</code> to them after applying the substitution corresponding to <code class="verbatim">S</code>. If <code class="verbatim">or</code> succeeds, it returns a new substitution set <code class="verbatim">S&#39;</code>, which can then be used for the remaining subgoals in <code class="verbatim">bigG</code>. If <code class="verbatim">and</code> succesfully goes through all subgoals, it returns a final set of substitution sets that essentially together contain the proof for <code class="verbatim">G</code>. The logic used for <code class="verbatim">unify</code> and <code class="verbatim">substitute</code> is really straightforward, so I won&#39;t go into that here. The notational quirk that I was somewhat unware of, is the use of the colon <code class="verbatim">:</code> to denote splitting a list&#39;s first element from the rest of the list. This is really quite similar to the way Haskell splits lists, with the notation <code class="verbatim">[x:xs]</code> used to denote the list as a whole but with the first element <code class="verbatim">x</code> and the rest of the list <code class="verbatim">xs</code> accessible as values.</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-6" class="outline-3">
<h3 id="headline-6">
Differentiable Prover
</h3>
<div id="outline-text-headline-6" class="outline-text-3">
<p>The main thing defined in this paper is the NTP (Neural Theorem Prover), which is a neural network that takes in a goal and tries to prove it using a modified version of prolog&#39;s backward chaining algorithm, and spits out a sucess score. NTPS are defined in terms of <em>modules</em>, which are subgraphs that are designed for one particular task that is part of a larger goal of the whole network. Each module takes atoms, rules, and a <em>proof state</em> as input, and returns a list of new proof states. A proof state is a tuple $S = (\psi, \rho)$, where $\psi$ is the substitution set constructed in the proof so far, and $\rho$ is a neural network that outputs a real valued success score of a partial proof. Once a module is constructed, it recursively instantiates submodules to continue the proof. The substitution set of a proof state $S$ is denoted $S_{\psi}$, and the corresponding neural network for calculating proof success is denoted $S_{\rho}$.</p>
<div id="outline-container-headline-7" class="outline-4">
<h4 id="headline-7">
Unification Module
</h4>
<div id="outline-text-headline-7" class="outline-text-4">
<p>One of the main modifications that NTPs make to the original backward chaining algorithm is that when unifying two atoms, symbol comparison is replaced with a computation that measures the similarity of the vector representations of those two symbols. The example used is the comparison of the predicates <code class="verbatim">grandfatherOf</code> and <code class="verbatim">grandpaOf</code>, which aren&#39;t symbolically the same, but which can have very close <em>learned representations</em>, using something like <a href="https://arxiv.org/pdf/1606.06357">ComplEx</a>. The <code class="verbatim">unify</code> module updates the input substitution set and creates a neural network for comparing vector representations of non-variable symbols in two sequences of terms (i.e. the terms in the two atoms). The module iterates pairwise through the terms of the two atoms being compared, and if one of the symbols is a variable, a substitution is added to the substitution set, and if they are both constants, their vector representations are compared using a <a href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel">Radial Basis Function Kernel</a>.</p>
<p>
The following is the pseudocode for the <code class="verbatim">unify</code> module taken directly from the paper:</p>
<p>
<img src="/unify.png" alt="/unify.png" title="/unify.png" /></p>
<p>
One thing that confused me about this is on line 4, where there seem to be two assignments. My educated guess is that they meant to say that $S&#39; = (S&#39;_{\psi}, S&#39;_{\rho})$, but accidentally ended up writing $\text{unify}_{\theta}(H, G, S&#39;) = (S&#39;_{\psi}, S&#39;_{\rho})$, since only the former makes sense as a recursive definition.</p>
<p>
Moving on to actually analyzing the code, the two things of significance are the definitions of $S_{\psi}&#39;$ and $S_{\rho}&#39;$. The main pseudocode is very similar to the backward chaining pseudocode from earlier, with a couple of subtle differences. One difference is that there is no pattern match for the case $\text{unify}_{\theta}(\_,\_,\text{FAIL})$, since the final pattern match never results in a call to $\text{unify}_{\theta}$ with $S&#39; = \text{FAIL}$ like it did in the original pseudocode. Another difference is that due to the new structure of $S$, the final case constructs the two different components of $S&#39;$, only one of which was seen in the previous case (namely, $S_{\psi}&#39;$). Another thing to notice is that $S_{\psi}&#39;$ does not result in $\text{FAIL}$ even if neither term is a variable and the terms aren&#39;t equal. This is because the $S_{\rho}$ component explicitly contains the calculation of a score for the cases when neither term is a variable, which is the term $$\exp\left(\frac{-||\mathbf{\theta}_{h:} - \mathbf{\theta}_{g:}||_{2}}{2\mu^2}\right)$$</p>
<p>
Observe that when $h$ and $g$ are the same, they will have the same vector representation, and thus $\mathbf{\theta}_{g:} = \mathbf{\theta}_{h:}$, which results in $S_{\rho} = e^0 = 1$. The futher apart the vectors $\theta_{g:}$ and $\theta_{h:}$ are (with repsect to the Euclidean metric), the larger the norm of their difference is, which in turn translates to a smaller value of $S_{\rho}&#39;$. In fact, the decay is exponential which means that only really similar vectors get high scores. Note that we are taking the minimum of this new score and the old score, which means that by the end of the algorithm, the remaining value of $S^{(n)}_{\rho}$ will be decided by the pair of terms furthest away from each other in their vector representations.</p>
<figure>
<img src="/exp_decay.png" alt="/exp_decay.png" title="/exp_decay.png" /><figcaption>
Credit: Desmos
</figcaption>
</figure>
<p>
Another observation pointed out in the paper is that with this new algorithm, the only cases where the $\text{FAIL}$ output can be achieved is when the two atoms don&#39;t have the same number of terms (i.e. arity mismatch).</p>
</div>
</div>
<div id="outline-container-headline-8" class="outline-4">
<h4 id="headline-8">
OR Module
</h4>
<div id="outline-text-headline-8" class="outline-text-4">
<p>The <code class="verbatim">or</code> module is defined as</p>
<p>
<img src="/or.png" alt="/or.png" title="/or.png" /></p>
<p>
The knowledge base, as a set of rules, is denoted by $\mathfrak{K}$. The <code class="verbatim">or</code> module in this case differs from the original symbolic <code class="verbatim">or</code> function in that it takes a new input, $d \in \mathbb{N}$, which defines the maximum proof depth of the neural network, and that it now uses the <code class="verbatim">unify</code> module, defined above, and the <code class="verbatim">and</code> module, defined below. The main difference between the symbolic and the neural <code class="verbatim">or</code> modules is that the neural module can capture similarities between different symbolic terms because it uses the neural <code class="verbatim">unify</code> module.</p>
</div>
</div>
<div id="outline-container-headline-9" class="outline-4">
<h4 id="headline-9">
AND Module
</h4>
<div id="outline-text-headline-9" class="outline-text-4">
<p>The <code class="verbatim">and</code> module is defined as</p>
<p>
<img src="/and.png" alt="/and.png" title="/and.png" /></p>
<p>
The new parameter, $d$, introduces one new case, where we automatically fail if $d = 0$. The main case (line 4) itself is different in that it uses the neural versions of <code class="verbatim">and</code> and <code class="verbatim">or</code> (the <code class="verbatim">substitute</code> function is actually the exact same as before), and makes sure that subsequent calls to <code class="verbatim">or</code> and <code class="verbatim">and</code> get lower proof depths.</p>
</div>
</div>
<div id="outline-container-headline-10" class="outline-4">
<h4 id="headline-10">
Final Model
</h4>
<div id="outline-text-headline-10" class="outline-text-4">
<p>The final aggregate model for proving a goal $G$ using a Knowledge Base $\mathfrak{K}$ with parameters $\mathbf{\theta}$ and proof depth $d$ is given by</p>
<p>
<img src="/final.png" alt="/final.png" title="/final.png" /></p>
</div>
</div>
<div id="outline-container-headline-11" class="outline-4">
<h4 id="headline-11">
Analysis of Final Model
</h4>
<div id="outline-text-headline-11" class="outline-text-4">
<p>The final model takes in two inputs, the goal $G$, and the maximum proof depth, $d$. It then iterates through all the successful solutions produced by $\text{or}_{\mathbf{\theta}}^{\mathfrak{K}}(G,d,(\varnothing, 1))$, and finds the one with the highest $S_{\rho}$ score. The call $or_{\mathbf{\theta}}^{\mathfrak{K}}(G,d,(\varnothing, 1))$ starts by constructing several <code class="verbatim">unify</code> modules, which are then all connected to <code class="verbatim">and</code> modules, which then goes through <code class="verbatim">substitue</code> before going back to <code class="verbatim">or</code> with depth $d - 1$. This continues until there is either a succesful solution, or if $d = 0$ or unification fails, which only happens if arity doesn&#39;t match.</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-12" class="outline-3">
<h3 id="headline-12">
Training
</h3>
<div id="outline-text-headline-12" class="outline-text-3">
<div id="outline-container-headline-13" class="outline-4">
<h4 id="headline-13">
Training Objective
</h4>
<div id="outline-text-headline-13" class="outline-text-4">
<p>The paper uses a <em>negative log-likelihood</em> loss function on the proof success score defined above. This paper also uses corrupted fact triples much in the same way that the paper on <a href="/technical/neural-tensor-kb-completion">NTNs</a> used them for training, with the main difference being that the corrupted data is explicitly given a score of $0$. The labeled training data is the set $\mathcal{T}$. The loss function is given by</p>
<p>
$$ \mathcal{L}_{\text{ntp}_{\mathbf{\theta}}^{\mathfrak{K}}} = \sum_{([s,i,j], y) \in \mathcal{T}} -y\log(\text{ntp}_{\mathbf{\theta}}^{\mathfrak{K}}([s,i,j],d)_{\rho}) - (1 - y)\log(1 - \text{ntp}_{\mathbf{\theta}}^{\mathfrak{K}}([s,i,j],d)_{\rho})$$</p>
<p>
where $[s, i, j]$ is an atom and $y$ is the labeled proof score, which is $1$ for original ground atoms and $0$ for the corrupted ones that were added in later.</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-14" class="outline-3">
<h3 id="headline-14">
Experimental Results
</h3>
<div id="outline-text-headline-14" class="outline-text-3">
<p>Looking at the table of results, NTP$\lambda$ (NTP combined with ComplEx) had comparable results with ComplEx across the board, even though the accuracy was slighly higher in NTP$\lambda$ for most metrics. The paper points out that one advantange that NTPs have is that they are more interpretable, in the sense that their induced rules can be examined.</p>
</div>
</div>
<div id="outline-container-headline-15" class="outline-3">
<h3 id="headline-15">
But what is &#34;End-To-End Differentiable&#34;?
</h3>
<div id="outline-text-headline-15" class="outline-text-3">
<p>End-To-End Differentiability, from what I have understood, refers to the fact that each of the modules within the larger <code class="verbatim">ntp</code> module has a derivative with respect to the vector representations of terms, making it possible to perform gradient descent on the loss. According to the appendix, the caveat is that the graph is so large that it becomes infeasible to backpropogate through it to get an exact gradient, which means that they resort to a heuristic of the gradient.</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-16" class="outline-2">
<h2 id="headline-16">
Final Thoughts
</h2>
<div id="outline-text-headline-16" class="outline-text-2">
<p>This paper was quite interesting and exposed me to several new concepts in automated reasoning and machine learning. I am not so sure whether this area has potential for future success, but it could remain in my purview. </p>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>A Summary of &#34;Reasoning With Neural Tensor Networks for Knowledge Base Completion&#34;</title>
      <link>http://localhost:1313/technical/neural-tensor-kb-completion/</link>
      <pubDate>Tue, 26 Nov 2024 01:00:00 -0600</pubDate>
      <guid>http://localhost:1313/technical/neural-tensor-kb-completion/</guid>
      <description>Here, I summarize and try to explain in detail what I read and understood in the paper entitled &amp;#34;Reasoning With Neural Tensor Networks for Knowledge Base Completion&amp;#34; by Socher, Chen, Manning, and Ng from Stanford.</description>
      <content:encoded><![CDATA[
<p>
Here, I summarize and try to explain in detail what I read and understood in the paper entitled <a href="https://proceedings.neurips.cc/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf">&#34;Reasoning With Neural Tensor Networks for Knowledge Base Completion&#34;</a> by Socher, Chen, Manning, and Ng from Stanford.</p>
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
Main Ideas
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<p>The overall goal of the paper is to answer whether two entities, $(e_1, e_2)$, are in a given relation $R$. </p>
<div id="outline-container-headline-2" class="outline-3">
<h3 id="headline-2">
Neural Tensor Network
</h3>
<div id="outline-text-headline-2" class="outline-text-3">
<p>This is a modified neural network architecture that has a <em>bilinear</em> tensor layer instead of a standard linear layer that directly relates the two entities. The aim of this model is compute a score that indicates how likely it is for the two entities to be in a given relationship. The function is defined by:</p>
<p>
$$g(e_1, R, e_2) = u^{T}_{R} f \left(e_1^T W_{R}^{[1:k]}e_2 + V_{R}\begin{bmatrix}e_1 \\ e_2\end{bmatrix} + b_R\right)$$</p>
<p>
$W_{R}^{[1:k]} \in \mathbb{R}^{d \times d \times k}$ is a tensor, and $e_{1}^{T}W_{R}^{[1:k]}e_2$ is what the paper calls a &#34;bilinear tensor product&#34; (I couldn&#39;t find a formal definition of this anywhere online), which is then added to the output of a standard layer, $V_R$, which is then added to the the bias, $b_R$. The whole sum is then passed through $f$, which is elementwise $\tanh$, and finally multiplied on the left by $u_R^{T}$, where $u_R$ is determines how the activated weights are combined to get a signle final score $g \in \mathbb{R}$.</p>
<p>
This equation seemed a bit daunting to me at first, so here&#39;s a more careful examination of what is going on:</p>
<p>
First, a reminder of what $\tanh$ looks like:</p>
<figure>
<img src="/tanh.png" alt="/tanh.png" title="/tanh.png" /><figcaption>
Credit: Desmos
</figcaption>
</figure>
<p>
The following sum is fed into an elementwise $\tanh$ that operates on a vector in $\mathbb{R}^{k}$:</p>
<p>
$$e_{1}^{T}W_{R}^{[1:k]}e_2 + V_{R}\begin{bmatrix}e_1 \\ e_2\end{bmatrix} + b_R$$</p>
<ul>
<li>The easiest to identify thing here is the bias node, which is represented by the vector $b_R$.</li>
<li>The next easy thing to identify here is the regular neural network layer, represented by the product $V_{R}\begin{bmatrix}e_1 \\ e_2\end{bmatrix}$, where $V_{R} \in \mathbb{R}^{k \times 2d}$ represents the weight matrix that specifies how to linearly combine the input in $k$ different ways. The vector $\begin{bmatrix}e_1 \\ e_2 \end{bmatrix}$ is just a singular vector in $\mathbb{R}^{2d}$ constructed by vertically concatenating the entries of $e_1$ and $e_2$ into one vector. So far so good. The expression $V_{R}\begin{bmatrix}e_1 \\ e_2 \end{bmatrix} + b_R$ itself is taken straight out of the expression for a single neural network layer in a classical neural network, where this sum is then passed through an activation function and then through the remaining layers.</li>
<li>All that remains to parse is the most interesting and different part of the sum, the bilinear tensor product, $e_{1}^{T}W_{R}^{[1:k]}e_{2}$. This notation was slightly confusing, but a diagram from the paper was illustrative: this operation represents stacking $k$ bilinear forms $e_1^{T}W_{R}^{i}e_2, i \in \{1, \ldots, k\}$ on top of each other to get a vector in $\mathbb{R}^{k}$. The tensor $W_{R}^{[1:k]}$ can be thought of as $k$ slices put together, where each slice is a $d \times d$ matrix relating entries from $e_1$ to entries in $e_2$. A concrete example might be of use: Let $d, k = 2$, let $e_1 = \begin{bmatrix}a \\ b\end{bmatrix}$ and let $e_2 = \begin{bmatrix}c \\ d\end{bmatrix}$. Let $W^{1}_{R} = \begin{bmatrix} w_{11}^1 &amp; w_{12}^1 \\ w_{21}^1 &amp; w_{22}^1 \end{bmatrix}$ and $W_{R}^{2} = \begin{bmatrix} w_{11}^2 &amp; w_{12}^2 \\ w_{21}^2 &amp; w_{22}^2 \end{bmatrix}$. Then $$W_{R}^{1}e_2 = \begin{bmatrix}w_{11}^{1}c + w_{12}^{1}d \\ w_{21}^{1}c + w_{22}^{1}d\end{bmatrix} \text{ and } W_{R}^{2}e_2 = \begin{bmatrix}w_{11}^{2}c + w_{12}^{2}d \\ w_{21}^{2}c + w_{22}^{2}d\end{bmatrix}$$</li>
</ul>
<p>Then</p>
<p>
$$e_{1}^{T}W_{R}^{1}e_2 = a(w_{11}^{1}c + w_{12}^{1}d) + b(w_{21}^{1}c + w_{22}^{1}d) $$</p>
<p>
and</p>
<p>
$$e_{1}^{T}W_{R}^{2}e_2 = a(w_{11}^{2}c + w_{12}^{2}d) + b(w_{21}^{2}c + w_{22}^{2}d)$$</p>
<p>
The interesting thing to note in both of these forms is that each entry in $e_1$ gets to be multiplied with each entry in $e_2$, and the product of any two individual entries is given a distinct weight. The final &#34;bilinear tensor product&#34; is then</p>
<p>
$$e_{1}^{T}W_{R}^{[1:2]}e_{2} = \begin{bmatrix}
w_{11}^{1}ac + w_{12}^{1}ad + w_{21}^{1}bc + w_{22}^{1}bd \\
w_{11}^{2}ac + w_{12}^{2}ad + w_{21}^{2}bc + w_{22}^{2}bd
\end{bmatrix}$$</p>
<p>
What I have understood through this example is that the bilinear tensor product term is just $k$ bilinear forms stacked on top of each other. What this means is that each entry in $e_1$ gets to be multiplied with each entry in $e_2$ $k$ times with $k$ different weights. Thus, we get to turn $k$ knobs, where a knob is a $d \times d$ matrix representing the strength of association between pairs of entries in $e_1$ and $e_2$. The paper explains that this bilinear term allows us the model to explicitly relate the two inputs multiplicatively, rather than just having an implict nonlinear association that we would get with this term removed.</p>
<p>
In summary, not only do we get to control how the stacked input vector is recombined, we also get to control how pairwise products of the vector entries are weighted.</p>
<p>
Finally, once the big sum is passed through the $\tanh$ activation function, the resulting $k$-vector gets multiplied by $u_{R}^{T}$, which is a row $k$-vector, thus giving us a single score at the very end.</p>
<p>
The paper points out that the Neural Tensor Network model, as defined above, combines the ideas and strengths from several different model types.</p>
</div>
</div>
<div id="outline-container-headline-3" class="outline-3">
<h3 id="headline-3">
Loss Function
</h3>
<div id="outline-text-headline-3" class="outline-text-3">
<p>
The loss function or training objective in this paper is called a &#34;contrastive max-margin&#34; objective function. The paper descrbes one main idea used to motivate this objective function: if we have a training set $T^{(i)} = (e_{1}^{(i)}, R^{(i)}, e_{2}^{(i)})$, each triplet that actually belongs to the training set should receive a higher score than a triplet where one of the entities is replaced randomly with a new entity. This seems like a natural requirement, since the relationships defined by triplets in the training set are <em>known</em> to be true. The triplets where an entity has been replaced by a random entity is called a <em>corrupted</em> triplet. The set of corrupted triplets is denoted by $T_{c}^{(i)} = (e_{1}^{(i)}, R^{(i)}, e_c)$. Here, $e_c$ has been randomly sampled from the set of all entities that can appear at that position in the relation $R^{(i)}$. (‼ one point I was confused about here was whether or not $e_c$ is parameterized by $i$. It seems like it should be, since the possible choices of $e_c$ depends on the relation $R^{(i)}$, which itself is indexed by $i$). What I found a little bit interesting here is that the corruption only happens in one position. A relation $R$ doesn&#39;t have to be symmetric, which means that a corruption $(e_1, R, e_c)$ is different from a corruption $(e_c, R_, e_2)$. Why, then, do we only corrupt on the right?</p>
<p>
As we saw earlier, the Neural Tensor Network model itself is parameterized by the choice of relation $R$, and in particular, each relation $R$ has its own set of weight matrices/tensors, $W_R, V_R, u_R, b_R$. Here, I faced another point of confusion. The paper defines $\mathbf{\Omega}$ to be the set of NTN parameters for <em>all</em> relationships, and it is comprised of $\mathbf{u}$, $\mathbf{W}$, $\mathbf{V}$, $\mathbf{b}$, and $\mathbf{E}$. While the first four of these are clear, I am a little confused about what $E$ is supposed to be. Is it the set of all entities? Finally, the paper defines the objective function as:</p>
<p>
$$J(\mathbf{\Omega}) = \sum_{i = 1}^{N}\sum_{c = 1}^{C}\max\left(0, 1 - g(T^{(i)}) + g(T_{c}^{(i)})\right) + \lambda ||\mathbf{\Omega}||_{2}^{2}$$</p>
<p>
Where $N$ is the number of training points, $C$ is the number of randomly sampled corrupted triplets of each given correct triplet (i.e. in the training set). The max in the summation forces the the minimizer to drive $g(T^{(i)})$ to be as much larger than $g(T_{c}^{(i)})$ as possible, up until it reaches exactly $1$ more than $g(T_{c}^{(i)})$, at which point any additional increase in $g(T^{(i)})$ is meaningless for the output $J$. The $\lambda ||\mathbf{\Omega}||_{2}^{2}$ summand is a standard $L_2$ regularization term that helps with overfitting.</p>
<p>
This equation for the objective function was a little puzzling initially, since it isn&#39;t quite clear what it means to take the $2$-norm of $\mathbf{\Omega}$, which itself wasn&#39;t defined very precisely. Though reading onto the paragrah after that reveals that this ambiguous notation is actually defining a set of five different objective functions (perhaps we can the final objective as the minimization of their sum?) This is still a point of slight unclarity for me. The paper uses the <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> nonlinear optimization method to find a local minimum of the cost function. </p>
</div>
</div>
<div id="outline-container-headline-4" class="outline-3">
<h3 id="headline-4">
Vector Representations
</h3>
<div id="outline-text-headline-4" class="outline-text-3">
<p>In the framework being used for this paper, each entity has a vector representation $e \in \mathbb{R}^d$. It seems like this framework was being used in multiple papers in the early 2010s, including in <a href="https://ronan.collobert.com/pub/2011_knowbases_aaai.pdf">&#34;Learning Structured Embeddings of Knowledge Bases&#34;</a> by Bordes, Weston, Collobert, and Bengio, in which a way of assigning entities vector representations is discussed. </p>
<p>
The NTN paper (the one currently being summarized) states that the NTN model works well with randomly initialized entity vectors, which are then learned for each entity through the training process (since the actually relationships between entity vectors are part of the traning data, which then translates to the learned function $g$). The paper also proposes a new scheme for representing entities using the composition of <em>word vectors</em>, which are vectors in $\mathbb{R}^d$. An entity is represnted by the average of the vectors of words that compose to it. For example, $v_{\textit{homo sapiens}} = 0.5(v_{\textit{homo}} + v_{\textit{sapiens}})$. This can then embed some similarities between entities before even training. The example used in the training is <em>homo erectus</em>. If this entity hasn&#39;t been seen before, a fact about <em>homo sapiens</em> can still be extended to it due to the fact that $v_{\textit{homo}}$ is in the word compositions for both vector representations, which means that $v_{\textit{homo erectus}}$ will start out relatively close to $v_{\textit{homo sapiens}}$ even though $v_{\textit{erectus}}$ is random.</p>
<p>
The total number of entities is $N_E$ and the total number of unique words is $N_W$. If the training is done on words, the entity embedding is $E \in \mathbb{R}^{d \times N_W}$ and if the training is performed with whole vectors, the entity embedding is $E \in \mathbb{R}^{d \times N_E}$. </p>
</div>
</div>
<div id="outline-container-headline-5" class="outline-3">
<h3 id="headline-5">
Experimental Results
</h3>
<div id="outline-text-headline-5" class="outline-text-3">
<p>The experiments performed in the paper were quite succesful, achieving accuracies of $86.2\%$ on the WordNet dataset and $90\%$ on the FreeBase dataset, though the improvement seemed marginal over an existing model called the Bilinear Model (not quite the same as the NTN, though it uses an idea that inspired the NTN).</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-6" class="outline-2">
<h2 id="headline-6">
Final Thoughts
</h2>
<div id="outline-text-headline-6" class="outline-text-2">
<p>This was my first look at Knowledge Base completion. I thought it was quite an interesting area and I might look further into it later. What brought me to this paper was the paper called <a href="https://arxiv.org/abs/1705.11040">End-To-End Differentiable Proving</a> by Rocktäschel and Riedel, which I wanted to study as a part of my dive into automated and neurosymbolic reasoning. I will attempt to summarize that paper next.</p>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>A Summary of PUTNAMBENCH</title>
      <link>http://localhost:1313/technical/putnam-bench/</link>
      <pubDate>Sun, 24 Nov 2024 16:52:52 -0600</pubDate>
      <guid>http://localhost:1313/technical/putnam-bench/</guid>
      <description>A summary of the &lt;a href=&#34;https://arxiv.org/abs/2407.11214&#34;&gt;PUTNAMBENCH&lt;/a&gt; paper by Tsoukalas et al. In this paper, the authors formalized hundreds of problems from the William Lowell Putnam Mathematical Competition in order to test the capabilities oe modern &lt;em&gt;neural models&lt;/em&gt; in proving theorems in the framework of theorem provers such as Lean 4, Isabelle, and Coq are tested. These frameworks can automatically and rigorously verify the correctness of the proofs provided by the neural models.</description>
      <content:encoded><![CDATA[
<p>
While looking at computer science research areas that I could find interesting, I stumbled upon formal methods, and more specifically, automated symbolic reasoning, theorem proving, and the integration of modern machine learning with formal reasoning. I decided to read some research papers to get a feel for this area, since it looked quite interesting to me. In this article, I am going to review and outline one interesting paper I read in this area. I will continue writing further articles about other papers I read.</p>
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
PUTNAMBENCH
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<p>The first interesting paper I stumbled upon was the <a href="https://arxiv.org/abs/2407.11214">PUTNAMBENCH</a> paper by Tsoukalas et al., where the capabilities of modern <em>neural models</em> in proving theorems in the framework of theorem provers such as Lean 4, Isabelle, and Coq are tested. These frameworks can automatically and rigorously verify the correctness of the proofs provided by the neural models. In this paper, the authors formalized hundreds of problems from the William Lowell Putnam Mathematical Competition. </p>
<p>
The improvement that PUTNAMBENCH makes on existing benchmarks is that it introduces college level problems into the mix, with some problems even requiring ideas from research level mathematics, according to the paper. A few additional reasons cited for the creation of this benchmark were:</p>
<ul>
<li>The limited scope of existing benchmarks</li>
<li>Existing benchmarks being designed for older frameworks</li>
<li>Preventing the leakage of benchmark data into the training data for LLMs (in general, the paper claims that this necessitates periodically creating new benchmarks)</li>
</ul>
<p>One issue that PUTNAMBENCH had to address was that Putnam problems often aren&#39;t stated as logical propositions. In fact, more often than not, they require the student to both come up with a closed form solution and then prove that the solution is indeed correct. PUTNAMBENCH addresses this issue by splitting up generation of closed form solutions from the proofs of correctness into two tasks of different difficulty levels, where success in one task likely has high correlation with success in the other. The second task only asks for a proof of correctness of a pre-provided closed form solution. The first task is a strict superset of the second task, since it requires not only the generation of a closed form solution, but also a proof of correctness.</p>
<p>
PUTNAMBENCH is claimed in the paper to be the first formalization of a large number of Putnam problems in Lean, Isabelle, or Coq, which is what is used to justify the idea that there isn&#39;t much cross-contamination between the dataset produced by the paper and the data used by large language models for training. I found this to be an interesting claim. Large language models probably have seen Putnam problems and their solutions in their natural language forms, but the claim that they haven&#39;t been exposed to formalizations of these problems and their proofs does seem plausible. It is then an interesting question whether or not seeing the natural language variants would give a language model an unfair advantage in the solving of the formalizations. The paper does acknowledge the possibility of such an indirect form of contamination.</p>
<p>
The results of running various theorem proving models on these formalizations was quite astonishing to me, as a newcomer to this field. None of provers were able to get more than even a handful of the problems. I don&#39;t know whether this is typical of benchmarks for formal theorem proving, but it was a surprise to me. It also indicated to me that there is much progress left to be made in this area.</p>
<p>
After reading this paper, I was curious to learn about the current state of the art in neurosymbolic reasoning. I wanted to learn how some of the models used (though unsuccesfully) in the PUTNAMBENCH paper worked. I therefore started reading some papers on this area. I also wanted to a bit about theorem proving frameworks, so I also began reading about those.</p>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Missing Out on The Old Internet</title>
      <link>http://localhost:1313/ramblings/the-old-internet/</link>
      <pubDate>Sun, 24 Nov 2024 00:11:15 -0600</pubDate>
      <guid>http://localhost:1313/ramblings/the-old-internet/</guid>
      <description>One of the reasons I decided to start writing a blog this year was because I was inspired by an old blog I found while watching Code Geass all the way back in December last year. I wanted to see other people&amp;#39;s opinions on Code Geass, and I stumbled upon a seemingly &lt;em&gt;ancient&lt;/em&gt; blog that chronicled the release of every new episode of CG while it was &lt;em&gt;airing&lt;/em&gt;. Something about the blog really pulled me in. I read some other posts on that blog from that era—mostly about releases of new techonology—and it transported me to a bygone era of the internet.</description>
      <content:encoded><![CDATA[<p>
One of the reasons I decided to start writing a blog this year was because I was inspired by an old blog I found while watching Code Geass all the way back in December last year. I wanted to see other people&#39;s opinions on Code Geass, and I stumbled upon a seemingly <em>ancient</em> blog that chronicled the release of every new episode of CG while it was <em>airing</em>. Something about the blog really pulled me in. I read some other posts on that blog from that era—mostly about releases of new techonology—and it transported me to a bygone era of the internet.</p>
<p>
The internet that I grew up on (I mainly started using the internet extensively around 2013 onward) was quite a bit different from the internet that came just a few years before that. The rapidly changing nature of the web meant that by the time I got truly using the internet on an everyday basis, much of its original charm was gone. I&#39;m talking blogs, forums, IRC. Sure these things might exist to some extent today, but the landscape of today&#39;s internet looks vastly different. It seems that the functionality of blogs and forums has now been replaced by things such as Twitter, Reddit, and Youtube. The almost complete centralization of expression (there are exceptions) has, as it seems to me, made the internet lose the charm that it once had. When I visit old forums, I see that people were genuinely engaging with each other, talking sincerely about their interests. On Reddit, there are a hundred bots on every post. Everything is moderated to death, and barely anything seems genuine. It really doesn&#39;t feel like there are real people on the other side of the screen writing the things that are written on Reddit. Some of the smaller subreddits manage to replicate the feeling of old forums to some extent, but it&#39;s just not the same. I can&#39;t quite place my finger on it. Maybe I am just romanticizing an era that I wasn&#39;t a part of, much the same way people like to think fondly of the past with a distorted lens.</p>
<p>
Whenever I find a blog that has been in writing for a very long time, I always find it really fun and interesting to go all the way back to the very first post. A couple of blogs that come to mind are from professors, such as the <a href="https://scottaaronson.blog/">Shtetl-Optimized</a> blog by Dr. Scott Aaronson, or the <a href="https://golem.ph.utexas.edu/category/">n-Category Café</a> by several professors from different universities. Visiting the posts from the early days of these blogs gives me a glimpse into the past. It really feels like a completely different internet from what we have now. People don&#39;t interact like that on the internet nowadays. There&#39;s some excitement and a feeling of freshness to those posts. Now that online social conventions have matured and become commonplace, we don&#39;t get to see comments like that. It seems like the more the internet has become governed by algorithms and attention maximization, the less it is a place of novel curiosity and intrigue. It may seem like a cliché, but it does seem like the internet is a less fun place to just &#34;hang out&#34; in these days. Given how many applications are doing everything in their power to suck us into a cycle of infinite scrolling, it seems like messaging people and finding relevant information will soon be just a few of the remaining things worth doing on the internet.</p>
<p>
In the past few months, I was reading the entirety of the <em>dragon ball</em> manga, and I frequently found myself asking random questions about plot points and power scaling, like a typical dragon ball fan. I kept finding myself returning to a website called <a href="https://www.kanzenshuu.com/">Kazenshuu</a>, where I frequently found very old posts about the exact issues and questions I had with and about the manga. It seemed like a really cool place for &#34;real&#34; dragon ball fans to hang out and talk. It seemed to have a lot more passion and enthusiasm about the series than the equivalent Reddit communities. It seemed like something that should exist for every hobby. Of course, I have visited my fair share of forums for things like <em>Gentoo</em> and <em>Arch</em> linux, and things like GameFAQs, but it really seems like something that could be much more commonplace for all sorts of hobbies. Now, I could just be ignorant, and these places probably do exist, but I do think that things like Reddit take away from the popularity of forums. I would really like to see a widespread resurgence in the popularity of individual forums for different fandoms and hobbies, and a move away from centralized platforms like Twitter and Reddit. Given how Reddit has shown its true colors over the last few years, it would probably be the best for everybody if forums became popular again.</p>
<p>
All in all, what I would really love to see a resurgence of is a <em>personal internet</em>, where more people have their own websites and more people put out their own content rather than just consuming things that are being shoved in their faces. Of course, this is incredibly unlikely given the direction that we seem to be headed in, but one can hope…</p>
]]></content:encoded>
    </item>
    <item>
      <title>A Review of Org Roam and Org Roam UI</title>
      <link>http://localhost:1313/reviews/org-roam-review/</link>
      <pubDate>Sat, 23 Nov 2024 22:04:12 -0600</pubDate>
      <guid>http://localhost:1313/reviews/org-roam-review/</guid>
      <description>I have been using the &lt;a href=&#34;https://www.orgroam.com/&#34;&gt;Org Roam&lt;/a&gt; package on emacs on and off for the last semester or so for taking notes for various subjects including compilers, point set (and a little bit of algebraic) topology, abstract algebra, some measure theory, machine learning, and even neuroevolution. Org roam is based on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Zettelkasten&#34;&gt;zettelkasten&lt;/a&gt; philosophy of notetaking, which has been explored and talked about by many people on the internet, so I won&amp;#39;t go into that here. Instead, I have gathered some of my thoughts after some use of org roam and I will be writing them here.</description>
      <content:encoded><![CDATA[
<p>
I have been using the <a href="https://www.orgroam.com/">Org Roam</a> package on emacs on and off for the last semester or so for taking notes for various subjects including compilers, point set (and a little bit of algebraic) topology, abstract algebra, some measure theory, machine learning, and even neuroevolution. Org roam is based on the <a href="https://en.wikipedia.org/wiki/Zettelkasten">zettelkasten</a> philosophy of notetaking, which has been explored and talked about by many people on the internet, so I won&#39;t go into that here. Instead, I have gathered some of my thoughts after some use of org roam and I will be writing them here.</p>
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
The Benefits
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<div id="outline-container-headline-2" class="outline-3">
<h3 id="headline-2">
Motivation
</h3>
<div id="outline-text-headline-2" class="outline-text-3">
<p>One of the most immediate and apparent benefits of using org roam has almost nothing to do with the methodology of zettelkastens themselves, but more so with the way the <a href="https://github.com/org-roam/org-roam-ui">Org Roam UI</a> package allows org roam users to visualize their graph of org roam nodes. Org roam UI displays all nodes and their connections in a really nice interactive graph with lots of theming and display options. It also comes with a really nice physics engine that allows the user to drag nodes around for cool visual effects. For me, one of the nonobvious benefits of this package has actually been the ability to see my web of notes grow larger and more dense over time. It can really be a source of motivation to write more notes and try to find as many connections between concepts as possible, because connections in concepts directly translate to more densely connected areas in the graph. The drive to find as many connections as possible is then itself very helpful for the retention of information, since each connection forces me to revisit an old concept and see how it relates to the new concept I am writing about. The whole system really helps reinforce different topics altogether and allows me to see how things are connected in the big picture. </p>
<figure>
<img src="/orui.png" alt="/orui.png" title="/orui.png" /><figcaption>
My Current Org Roam UI Graph
</figcaption>
</figure>
</div>
</div>
<div id="outline-container-headline-3" class="outline-3">
<h3 id="headline-3">
Connections
</h3>
<div id="outline-text-headline-3" class="outline-text-3">
<p>The most obvious point of connection between topics is the <em>set</em>. The set is one of the most primitive objects in modern mathematics and is used in essentially every field of study that uses mathematics to model something. With this, we must realize that connections that go through the set node are nearly meaningless. They don&#39;t really say much about how two concepts are connected. The same is true about <em>functions</em>. What is more interesting is when two areas in the graph such as topology and algebra are connected nontrivially through a node such as the <em>isomorphism</em> node. Isomorphisms are normally defined on <em>groups</em>, <em>rings</em>, and similar algebraic structures, but <em>homeomorphisms</em> can themselves be viewed as isomorphisms of <em>topological spaces</em>, since they are structure preserving maps. Perhaps such generalized connections would be more abundant with category theory, when I get to learning it. Of course, there are literal connecting points between algebra and topology too, like the fundamental group.</p>
</div>
</div>
<div id="outline-container-headline-4" class="outline-3">
<h3 id="headline-4">
Typesetting
</h3>
<div id="outline-text-headline-4" class="outline-text-3">
<p>This isn&#39;t really much of a benefit as much as it is an essential feature. If emacs didn&#39;t have in-buffer LaTeX previews or if org roam ui didn&#39;t have the ability to display LaTeX using katex, I would simply have not used these packages. Much of what I take notes on has mathematical notation all over the place, so it is crucial for any notetaking app to be able to display math.</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-5" class="outline-2">
<h2 id="headline-5">
The Drawbacks
</h2>
<div id="outline-text-headline-5" class="outline-text-2">
<div id="outline-container-headline-6" class="outline-3">
<h3 id="headline-6">
Poor Conventional Notetaking
</h3>
<div id="outline-text-headline-6" class="outline-text-3">
<p>The most apparent drawback with using org roam has been my inability to map conventional notes onto this system in an easy manner. The thing with educational content, be it in lecture form or textbook form, is that it hardly lends itself to be sorted into nice chunks separated by specific concepts. I have often experienced attending a lecture or reading a textbook where an entire section is dedicated to the development of multiple intertwining ideas altogether, and this really doesn&#39;t work well with the structure of dividing things up into individual concepts that are then connected with links. Perhaps this actually isn&#39;t as much of a downside as I think it is. There does seem to be some merit to the idea that if I am forced to convert standard expository material into this format, I am much more likely to absorb it. I would need to know how to split up a section into different org roam nodes, which would then force me to truly understand what is going on and how the different concepts are related. Another thing that I haven&#39;t actually explored is the daily note functionality, which might actually be better suited for first writing down whatever is coming at my way from textbooks or lectures. I could then sift through the lecture notes and properly organize them into final org roam nodes. This would perhaps actually help my retention and understanding of the topic by forcing me to revisit everything that I write down. </p>
<p>
Coming back to the point, org roam doesn&#39;t map as easily to conventional notetaking as regular org mode does, and requires some additional conversion work. Perhaps that is a good thing in the long run if one can stick to it.</p>
</div>
</div>
<div id="outline-container-headline-7" class="outline-3">
<h3 id="headline-7">
LaTeX Notetaking is Suboptimal in General
</h3>
<div id="outline-text-headline-7" class="outline-text-3">
<p>Another thing to note is that LaTeX notetaking, and probably any typesetting based notetaking in general, is quite suboptimal in terms of ease of use when compared to good old pen and paper. I find that mathematical concepts flow quite a bit more smoothly when I am using pen and paper. There is much less friction when converting an abstract idea in my head to symbolic notation on paper. There is an obvious tradeoff here: we are trading ease of use for ease of access (due to search and hyperlinks) and easier preservation (making a backup is trivial compared to pen and paper).</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-8" class="outline-2">
<h2 id="headline-8">
Final Thoughts
</h2>
<div id="outline-text-headline-8" class="outline-text-2">
<p>This experiment of using org roam has been fairly succesful and helped me take more notes this semester than any semester that came before. By rewarding me for taking notes and making connections, org roam has made studying and reviewing a lot more fun than it used to be. I have seen what some other people&#39;s UI graphs look like, and mine completely pales in comparison. I hope to have a respectable graph by the end of the Spring 2025 semester.</p>
</div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>The Pernicious Gum</title>
      <link>http://localhost:1313/short-stories/the-pernicious-gum/</link>
      <pubDate>Tue, 19 Nov 2024 12:24:20 -0600</pubDate>
      <guid>http://localhost:1313/short-stories/the-pernicious-gum/</guid>
      <description>As soon as he got a break from his studying, he walked over to the closest grocery store and started looking for gum. Something was off. Marary usually went for fruit or mint flavored gum that wasn&amp;#39;t too sweet. Today, however, no matter where he looked, all he could see was sour gum. He scanned every shelf in the gum section to no avail. Being somewhat pressed for time, Marary grabbed a pack of sour gum without looking, thinking that he would come back later to stock up on more acceptable flavors.</description>
      <content:encoded><![CDATA[<p>
<img src="/gum.png" alt="/gum.png" title="/gum.png" /></p>
<p>
It was a chilly November evening, and Marary Saina was studying for his topology final as he chewed on the last piece of gum he had left. He often chewed on gum as a stress-relief mechanism and always kept a spare pack of it at hand. This time, however, he had forgotten to restock gum in his last trip to the grocery store.</p>
<p>
As soon as he got a break from his studying, he walked over to the closest grocery store and started looking for gum. Something was off. Marary usually went for fruit or mint flavored gum that wasn&#39;t too sweet. Today, however, no matter where he looked, all he could see was sour gum. He scanned every shelf in the gum section to no avail. Being somewhat pressed for time, Marary grabbed a pack of sour gum without looking, thinking that he would come back later to stock up on more acceptable flavors.</p>
<p>
He came back home, opened the pack of gum, popped two pieces in his mouth, and went back to his desk to continue studying. After about ten minutes of chewing, Marary felt a mild sense of pain in his canines. He had expected this. This had been his experience with sour hard candy, and it was part of the reason he avoided sour candy and gum like the plague. This time, however, his need for gum had exceeded his distaste for sour acidity. He chewed on through the pain and kept working on practice problems. After about 30 minutes, the gum had lost even the final remaining dregs of flavor, and Marary was ready to spit it out and go for the next couple of pieces. As soon as he spit out the gum, a sharp, stinging pain came onto all four of his canines. The pain subsided within a few seconds, but it left Marary thoroughly shaken. He decided not to go for another piece, and continued studying.</p>
<p>
As he tried to complete more practice problems, he realized that his concentration had been completely disturbed. Something didn&#39;t feel right. He kept thinking about the gum. Though sour candy had usually caused him mild pain and trouble, the stinging pain at the end was like nothing he had ever experienced. Marary thought about the contents of the gum. Citric acid. Though citric acid was present in other commonplace foods such as oranges and lemons, Marary felt that the the citric acid in the gum was especially corrosive. Initially, he dismissed it as an irrational thought. But the longer he tried to ignore these thoughts, the more impairing they became. He kept thinking about how even though he had spit out the gum, its flavoring must have remained on his teeth. He imagined the enamel on his teeth slowly corroding away, exposing the incredibly sensitive nerve endings underneath. Just the thought of food touching those nerve endings made Marary pull his lips inward.</p>
<p>
He couldn&#39;t take it anymore. Marary jumped out of his chair and ran for the sink in his bathroom. He started vigorously rinsing his mouth with water. A minute later, he stopped and immediately started thinking about whether the water coming out of his sink was neutral. It hadn&#39;t occurred to him that the tap water might itself be slightly acidic. He ran to the nearest drugstore and came back with a pH test strip, and dropped some tap water on it, and compared the color to the provided scale. It was a pH of 6.5. Not good, he thought. He vaguely remembered hearing in his chemistry class that a pH below 7 was considered acidic. Marary started to panic. If the water itself was acidic, it clearly wouldn&#39;t be able to neutralize the acid from the gum. He tried to remember alkaline foods. Nuts, spinach, broccoli. He ran to his pantry and grabbed a fistful of almonds and started eating them. He calmed down, for the moment. He composed himself, and continued working on his problems.</p>
<p>
As the night got darker, Marary started getting drowsy. He thought that he had studied quite enough, and he decided to take a warm shower before heading to bed. Thirty seconds after getting into the shower, Marary accidentally got some of the shower water into his mouth. He suddenly remembered the pH reading from before, and at that moment the sharp, stinging pain came back to him as though he had been struck by lightning. Marary jumped out of the shower, turned off the water, and rushed to eat more almonds. At this point, he was convinced that the amount of time his teeth had spent drenched in acid was enough to have caused significant damage to the enamel of his teeth. He worried that any more acidity would lead to his teeth corroding entirely. He decided to visit the dentist&#39;s office the very next day to make sure that his suspicions were correct. He called to see if they had an appointment, and scheduled one for the morning of the next day.</p>
<p>
With that, Marary made his way to bed. He turned off the lights, pulled up his blanket, and turned to his right side in order to fall asleep. He couldn&#39;t fall asleep, though. He was restless. In his anxiety, he accidentally clenched his teeth. Sharp pain went up all four of his canines again and he opened his eyes wide. He was sure that he had irreparably damaged his teeth. He placed his index finger on his upper-right canine, and felt it dampen. He immediately turned on the bedroom lights, and stared at his index finger. His finger was drenched in what seemed to be blood from his gums. He touched his canine again, only to get another unbearable sensation of stinging pain, and more blood on his finger. He swore he had also felt his canine starting to move. He went to the bathroom, turned on the lights, and looked into the mirror with an open mouth. Nothing. There was nothing to be seen on his teeth. They all seemed to be perfectly positioned in his mouth with no signs of bleeding or movement.</p>
<p>
Marary was bewildered. He swore his gums had been bleeding and his canine nerves had been exposed and his tooth had been on the verge of completely falling off. He was afraid that he had no way of convincing anyone that this had been the case if he could not even convince himself with a mirror. With an upset mind and a shaken resolve, he went back to bed.</p>
<p>
As soon as his teeth touched each other, they started hurting again. He tried to ignore them, but the pain just wouldn&#39;t stop. He eventually decided to just keep his mouth open all night. But whenever he was about his fall asleep, his mouth would inadvertently seal shut and he would be awoken with a sharp pain. At this point, Marary was exhausted, frustrated, and sleep deprived. He no longer had the will to keep ignoring the pain. He decided to try one last thing: he got a large, thick ball of cotton, and placed it in his mouth in a way that would prevent his upper and lower teeth from touching each other. He felt somewhat content with this arrangement, and started to doze off. </p>
<p>
Soon his saliva started soaking the cotton, making it thinner and thinner. Given the cold weather, Marary&#39;s nose had gotten quite stuffy. In an attempt to breathe in more air, his mouth involuntarily opened itself and sucked in a whole bunch of air, causing the cotton to be pulled right into his throat, causing him to wake up with a start, gasping for air. Marary was startled. Being in a half asleep state, he had almost completely forgotten about his teeth. He spit out the ball of cotton and accidentally clamped down his teeth. In a sudden flash, he remembered about his teeth, and the pain came rushing back.</p>
<p>
At this point, Marary was devastated. He was on the verge of tears, and he wanted to pull his hair out. He thought his teeth were surely so corroded at this point, that even a contact with air would trigger the extremely vulnerable nerve endings on his teeth. Marary wanted to cry. He wished he had never gotten the sour gum in the first place. He didn&#39;t care about preserving his teeth anymore. He knew that they had been completely ruined at this point. He just wanted to be free of the pain. He started rummaging around his house, trying to find something that might help him. He laid his eyes on the perfect tool. The next day, Marary called the dentist&#39;s office to tell them that he would no longer need their services. However, they couldn&#39;t quite make out what he was trying to say.</p>
]]></content:encoded>
    </item>
    <item>
      <title>My Utter Disappointment with Trigun</title>
      <link>http://localhost:1313/reviews/trigun-review/</link>
      <pubDate>Tue, 19 Nov 2024 12:03:11 -0600</pubDate>
      <guid>http://localhost:1313/reviews/trigun-review/</guid>
      <description>When you listen to the &lt;em&gt;Trigun&lt;/em&gt; OP, you are electrified. The energetic guitar riffs, combined with the cool visuals that make the main character look like a badass, really got me pumped for the show ahead of me. Add to that the near-legendary reputation &lt;em&gt;Trigun&lt;/em&gt; has garnered on internet Anime spaces, and you have a show that may only live up to its expectations through incredible quality and depth. &lt;em&gt;Trigun&lt;/em&gt; is neither of those. In fact, it&amp;#39;s not even a good show.</description>
      <content:encoded><![CDATA[
<p>
<img src="/trigun-characters.png" alt="/trigun-characters.png" title="/trigun-characters.png" /></p>
<div id="outline-container-headline-1" class="outline-3">
<h3 id="headline-1">
Warning: Heavy spoilers for <em>Trigun</em>
</h3>
<div id="outline-text-headline-1" class="outline-text-3">
<p>
When you listen to the <em>Trigun</em> OP, you are electrified. The energetic guitar riffs, combined with the cool visuals that make the main character look like a badass, really got me pumped for the show ahead of me. Add to that the near-legendary reputation <em>Trigun</em> has garnered on internet Anime spaces, and you have a show that may only live up to its expectations through incredible quality and depth. <em>Trigun</em> is neither of those. In fact, it&#39;s not even a good show. Given how highly it is regarded on the internet, this take might be seen as blasphemous. However, a closer look at the show reveals that not only does this show have little of substance to offer with its writing, it isn&#39;t even passable as an action-packed turn-your-brain-off show. The show has problems in many aspects, from the seriously underdeveloped protagonist, to the annoying and unnecessary side characters that tag along on every journey, to the seemingly aimless plot the show has for the first half or so, to the unconvincing villain and surface level philosophy, to the serious lack of world building and satisfying answers to the mysteries set up in the show. The best word that comes to mind when describing <em>Trigun</em> is &#34;undercooked&#34;.</p>
</div>
</div>
<div id="outline-container-headline-2" class="outline-2">
<h2 id="headline-2">
The Protagonist
</h2>
<div id="outline-text-headline-2" class="outline-text-2">
<p>Vash The Stampede is easily the most iconic figure in the <em>Trigun</em> canon and is the focus of many of the art pieces that have come from the franchise. Many fans cite Vash as the reason for their enjoyment of the show. For me, however, Vash seems to not only be a not great protagonist, he actively makes the show worse. Initially, it isn&#39;t easy to accept that Vash, for the majority of the show&#39;s runtime, isn&#39;t the badass he is made out to be by the show&#39;s OP. After accepting this fact, however, you would still want Vash to be a well written, fleshed out character with flaws that are explored well in the show. You would at least expect his motivations to be reasonably derived from his backstory, and you would expect his philosophy to be challenged at some point in the show. Alas, all of these things either don&#39;t happen at all, or happen in the most shallow and half-assed way possible.</p>
<p>
We start the show off with a goofy Vash who is very reluctant to use any violence at all, even for self-defense. This is a good enough start, and sets up a mystery as to why he is so strongly against using any form of violence. We get one episode exploring Vash&#39;s backstory, where some lame anecdote about trying to save both a spider and its victim butterfly is used to show Vash&#39;s conviction from a young age. The only real reason he seems to think this is due to Rem&#39;s influence. This is the only episode we get about Vash&#39;s backstory, and leaves much to be desired in terms of justifying his philosophical convictions. The only other part that tries to explain his unwillingness to use violence is the fact that his arm can somehow grow a nuke that can cause great devastation, and that it has caused such devastation of human life in the past. This was also an interesting thread and could have been explored really well. But all we get are some vague flashbacks and trauma, leaving the viewer desiring for more context about the nuke. There is zero information about why his hand can become a nuke at all.</p>
<p>
Another pain point regarding Vash is the utter lack of growth. He begins thinking that all forms of violence are bad and is never seriously challenged about this belief. Even when he does end up killing a character, that character happens to be a genocidal maniac who plans to kill Vash&#39;s friends. There are no real moments where Vash has to make a morally grey decision by applying violence to the detriment of someone who isn&#39;t completely evil. Even when he does kill the one person he kills, he spends an entire episode agonizing over it, but comes out none the wiser from the arc. What is even more absurd is when he blames himself for the priest killing a literal Gung-Ho-Gun who would&#39;ve otherwise killed all the orphans. Vash&#39;s broken moral code is never properly challenged, and it leaves a sour taste in the viewer&#39;s mouth by the end of the show. The whole dilemma of having to choose between saving the butterfly and saving the spider is never explored. In the end, Vash does choose to kill Knives to save his friends, so that is at least some sort of resolution to the dilemma (i.e. kill the spider, but only if the other option is mass genocide), but it doesn&#39;t feel well explored.</p>
</div>
</div>
<div id="outline-container-headline-3" class="outline-2">
<h2 id="headline-3">
The Story
</h2>
<div id="outline-text-headline-3" class="outline-text-2">
<p>Any great show needs at least a good story. There can be shows that are enjoyable without a good story, but I would argue that they are undeserving of the level of admiration received by <em>Trigun</em>. Unfortunately, <em>Trigun</em> is neither enjoyable, nor does it have a good story. It certainly doesn&#39;t deserve the admiration it gets. <em>Trigun</em>&#39;s story is an unbearable slew of completely pointless episodic stories that border on being filler with unbaked plot points sprinkled here and there with no real overarching conflict, culminating in a rushed final arc that doesn&#39;t do the potential of the setup justice. There were several episodes that could have been completely done away with without changing the story at all. These episodes could have easily been used to instead expand on few plot points that had been set up. Let&#39;s take the bulbs for example. The <em>Trigun</em> world seems to have these strange bulbs that seem to power cities, and Vash strangely has the power to somewhat manipulate these bulbs. These bulbs are barely shown at all, and there is essentially no explanation for why they exist, how they function, and how they play into the larger story. They just exist as a cool decoration to the world, but don&#39;t provide much of substance. Similarly, the nuke on Vash&#39;s hand is given absolutely no explanation. Just one episode explaining its existence would have greatly improved the world building in the show. Another thing that could have had an entire episode dedicated to it was the flashback Vash has, very he is holding a young girl who has just experienced the devastation of her town. It is quite obvious that this destruction was caused by Vash&#39;s nuke in some way, but it would have been an excellent addition to the show if they had spent an entire episode properly going through the events of this flashback. It would maybe be acceptable to not have such an episode in another show, where the plot is so tightly written that it leaves no room for unnecessary expository episodes, but in a show like <em>Trigun</em>, where half the episodes are pointless, the creators could have easily fit in an entire telling of the tragic event.</p>
<p>
The most disappointing part of the story is the development of the conflict between Vash and Knives. <em>Trigun</em> has <strong>one</strong> episode dedicated to the development of Vash&#39;s backstory, the development of his motivations as a character, his relationship with Knives, and the beginning of the civilization on the planet they lived on. One. They tried to fit the most interesting and important bits of the story in one single episode, and it went just as well as could be expected. Knives&#39; hate for humanity felt unearned, his sudden turn as a villain felt forced, and the time we got with Rem felt 10 episodes too short. Vash&#39;s relationship with Rem felt incredibly underdeveloped from the few snippets we got in the episode. The episode left me with several more questions than I had before the episode. Making the viewer ask even more questions can actually be a great thing when done correctly. It can help build suspense and mystery, and significantly increase the viewer&#39;s interest in continuing the story. When done correctly, mysteries that are set up early on in the story can feel incredibly rewarding when they are resolved. The problem with <em>Trigun</em> is that for every mystery it resolves, it creates ten more that never see the light of day after their creation. It wouldn&#39;t even be such a problem if <em>Trigun</em> had an otherwise good plot, but the fact that the <strong>entire</strong> plot rested on this particular backstory made the fact that they rushed it even more egregious. It isn&#39;t like they had no episodes to spare for a better development of this backstory either. With the countless episodes spent with Vash going on borderline filler adventures with the insurance girls, the creators of the show had absolutely no reason not to dedicate a few of those episodes to a better development of the plot. Speaking of the insurance girls…</p>
</div>
</div>
<div id="outline-container-headline-4" class="outline-2">
<h2 id="headline-4">
The Side Characters
</h2>
<div id="outline-text-headline-4" class="outline-text-2">
<p>There is no way of sugarcoating it, the side characters in this show are almost completely unbearable (except for Wolfwood, the one saving grace). Both Meryl and Milly are one-note characters that spend the majority of their screentime acting out their roles as almost sitcom characters (in an unfunny sitcom). Merly is the &#34;serious&#34; and &#34;concerned&#34; woman who refuses to believe that Vash is the real Vash (a gag that the show repeated until it drained every ounce of comedic value it might have had when it was done initially). She has no humor throughout the show and goes through almost no development. Milly is slightly more bearable, though she too starts out as a caricature, a ditsy girl who doesn&#39;t understand the social norms of the world. She does show some improvement as a character, but she too is almost unbearable for first 60-70% of the show. Both the insurance girls are completely irrelevant to the plot, and the story wouldn&#39;t change even a single if they were removed. They simply serve as unsucessful comic relief, and could have easily been replaced by characters that had felt like real people with real emotions and growth. Wolfwood is the one character in the show that I found to be actually compelling. Though his backstory was a little bit cliched, his character really did have a real conflict. He was the one character who had to make morally tough decisions, and he felt like he was the only adult in the cast. A show where Wolfwood was the main the character would be significantly better than <em>Trigun</em>.</p>
</div>
</div>
<div id="outline-container-headline-5" class="outline-2">
<h2 id="headline-5">
The Villain(s)
</h2>
<div id="outline-text-headline-5" class="outline-text-2">
<p>The Gung-Ho-Guns have cool designs, and that&#39;s about it. Their abilties make no sense, their backstories and motivations are not compelling, and they don&#39;t work well as a coherent gang of villains. Knives is a terrible villain. He appears really late in the show, since the first half of the show is wasted by having Vash and co. go through incredibly unnecessary adventures. When he does appear, he isn&#39;t very compelling. Why does Knives hate humanity so much? How can he activate Vash&#39;s nuke? What even are Knives and Vash? Are they humans that have mutations? Are they aliens? Are they angels? The show leaves countless questions about Knives and Vash completely unanswered, and barely develops Knives&#39; motivation as villain. Now, villains that are evil for simple reasons can be done well, but they need the villainous acts to match. Knives barely appears in the show so we don&#39;t even get to see his villainous acts. Admittedly, the villains didn&#39;t annoy me nearly as much as the other aspects of the show. </p>
</div>
</div>
<div id="outline-container-headline-6" class="outline-2">
<h2 id="headline-6">
Conclusion
</h2>
<div id="outline-text-headline-6" class="outline-text-2">
<p>Maybe the <em>Trigun</em> manga expands on the mysteries that are set up. I&#39;ve read that the <em>Trigun</em> anime and manga are different enough that they should be considered separate works. Perhaps this is the case and perhaps the manga does do justice to its characters and story. But as I have laid out, the anime is far from that. The pacing is awful, the story fails to maintain momentum and fails to deliver on its mysteries, the characters and annoying and one-note, and the show doesn&#39;t even have proper badassery for the first half or so. For the amount of hype I had seen going into this show, I was expecting something on par with some of the all-time greats of anime such as <em>Akira</em> and <em>Monster</em>. I was prepared to be disappointed, but I still expected at least an entertaining show. So when I had to drag myself through the 23 episodes excruciatingly boring episodes (the backstory episode was quite good), I lost some of my faith in the opinions of anime fans. Oh, and <em>Trigun</em> has <strong>TWO</strong> recap episodes in its 26-episode run.  </p>
</div>
</div>
]]></content:encoded>
    </item>
  </channel>
</rss>
